{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 程序初始化\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "## 2. 预置训练参数初始化\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001)  # 学习率\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99)           # 经验折扣率\n",
    "parser.add_argument(\"--epochs\", type=int, default=10000)              # 迭代多少局数\n",
    "parser.add_argument(\"--buffer_size\", type=int, default=200000)      # replaybuffer大小\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)         # batchsize大小\n",
    "parser.add_argument(\"--pre_train_model\", type=str, default=None)   # 是否加载预训练模型\n",
    "parser.add_argument(\"--use_nature_dqn\", type=bool, default=True)   # 是否采用nature dqn\n",
    "parser.add_argument(\"--target_update_freq\", type=int, default=250) # 如果采用nature dqn，target模型更新频率\n",
    "parser.add_argument(\"--epsilon\", type=float, default=0.9)          # 探索epsilon取值\n",
    "args, _ = parser.parse_known_args()\n",
    "#！！！！注意参数是否实际使用，先读代码理解训练过程！！！！\n",
    "\n",
    "## 3. 创建环境\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = itertools.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "class IllegalMove(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def stack(flat, layers=16):\n",
    "    \"\"\"Convert an [4, 4] representation into [4, 4, layers] with one layers for each value.\"\"\"\n",
    "    # representation is what each layer represents\n",
    "    representation = 2 ** (np.arange(layers, dtype=int) + 1)\n",
    "\n",
    "    # layered is the flat board repeated layers times\n",
    "    layered = np.repeat(flat[:, :, np.newaxis], layers, axis=-1)\n",
    "\n",
    "    # Now set the values in the board to 1 or zero depending whether they match representation.\n",
    "    # Representation is broadcast across a number of axes\n",
    "    layered = np.where(layered == representation, 1, 0)\n",
    "\n",
    "    return layered\n",
    "\n",
    "def unstack(layered):\n",
    "    \"\"\"Convert an [4, 4, layers] representation back to [4, 4].\"\"\"\n",
    "    # representation is what each layer represents\n",
    "    layers = layered.shape[-1]\n",
    "    representation = 2 ** (np.arange(layers, dtype=int) + 1)\n",
    "\n",
    "    # Use the representation to convert binary layers back to original values\n",
    "    original = np.zeros((4, 4), dtype=int)\n",
    "    for i in range(layers):\n",
    "        # Convert the result to integer before adding\n",
    "        addition = (layered[:, :, i] * representation[i]).astype(int)\n",
    "        original += addition\n",
    "\n",
    "    return original\n",
    "\n",
    "\n",
    "# 游戏环境\n",
    "class Game2048Env(gym.Env):\n",
    "    metadata = {'render.modes': ['ansi', 'human', 'rgb_array']}\n",
    "\n",
    "    def __init__(self):\n",
    "        # Definitions for game. Board must be square.\n",
    "        self.size = 4\n",
    "        self.w = self.size\n",
    "        self.h = self.size\n",
    "        self.squares = self.size * self.size\n",
    "\n",
    "        # Maintain own idea of game score, separate from rewards\n",
    "        self.score = 0\n",
    "\n",
    "        # Members for gym implementation\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Suppose that the maximum tile is as if you have powers of 2 across the board.\n",
    "        layers = self.squares\n",
    "        self.observation_space = spaces.Box(0, 1, (self.w, self.h, layers), int)\n",
    "        self.set_illegal_move_reward(-20)\n",
    "        self.set_max_tile(None)\n",
    "\n",
    "        # Size of square for rendering\n",
    "        self.grid_size = 70\n",
    "\n",
    "        # Initialise seed\n",
    "        self.seed()\n",
    "\n",
    "        # Reset ready for a game\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def set_illegal_move_reward(self, reward):\n",
    "        \"\"\"Define the reward/penalty for performing an illegal move. Also need\n",
    "            to update the reward range for this.\"\"\"\n",
    "        # Guess that the maximum reward is also 2**squares though you'll probably never get that.\n",
    "        # (assume that illegal move reward is the lowest value that can be returned\n",
    "        self.illegal_move_reward = reward\n",
    "        self.reward_range = (self.illegal_move_reward, float(2 ** self.squares))\n",
    "\n",
    "    def set_max_tile(self, max_tile):\n",
    "        \"\"\"Define the maximum tile that will end the game (e.g. 2048). None means no limit.\n",
    "           This does not affect the state returned.\"\"\"\n",
    "        assert max_tile is None or isinstance(max_tile, int)\n",
    "        self.max_tile = max_tile\n",
    "\n",
    "    # Implement gym interface\n",
    "    def step(self, action):\n",
    "        \"\"\"Perform one step of the game. This involves moving and adding a new tile.\"\"\"\n",
    "        logging.debug(\"Action {}\".format(action))\n",
    "        score = 0\n",
    "        done = None\n",
    "        info = {\n",
    "            'illegal_move': False,\n",
    "        }\n",
    "        try:\n",
    "            score = float(self.move(action))\n",
    "            if score > 0:\n",
    "                score = score\n",
    "                # score = math.log2(score)\n",
    "            if score < 0:\n",
    "                score = 0\n",
    "            self.score += score\n",
    "            assert score <= 2 ** (self.w * self.h)\n",
    "            self.add_tile()\n",
    "            done = self.isend()\n",
    "            reward = float(score)\n",
    "\n",
    "        except IllegalMove:\n",
    "            logging.debug(\"Illegal move\")\n",
    "            info['illegal_move'] = True\n",
    "            done = True\n",
    "            reward = self.illegal_move_reward\n",
    "            # reward=0\n",
    "\n",
    "        # print(\"Am I done? {}\".format(done))\n",
    "        info['highest'] = self.highest()\n",
    "\n",
    "        # Return observation (board state), reward, done and info dict\n",
    "        return stack(self.Matrix), reward, done, info\n",
    "    def reset(self):\n",
    "        self.Matrix = np.zeros((self.h, self.w), int)\n",
    "        self.score = 0\n",
    "\n",
    "        logging.debug(\"Adding tiles\")\n",
    "        self.add_tile()\n",
    "        self.add_tile()\n",
    "\n",
    "        return stack(self.Matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Implement 2048 game\n",
    "    def add_tile(self):\n",
    "        \"\"\"Add a tile, probably a 2 but maybe a 4\"\"\"\n",
    "        possible_tiles = np.array([2, 4])\n",
    "        tile_probabilities = np.array([0.9, 0.1])\n",
    "        val = self.np_random.choice(possible_tiles, 1, p=tile_probabilities)[0]\n",
    "        empties = self.empties()\n",
    "        assert empties.shape[0]\n",
    "        empty_idx = self.np_random.choice(empties.shape[0])\n",
    "        empty = empties[empty_idx]\n",
    "        logging.debug(\"Adding %s at %s\", val, (empty[0], empty[1]))\n",
    "        self.set(empty[0], empty[1], val)\n",
    "\n",
    "    def get(self, x, y):\n",
    "        \"\"\"Return the value of one square.\"\"\"\n",
    "        return self.Matrix[x, y]\n",
    "\n",
    "    def set(self, x, y, val):\n",
    "        \"\"\"Set the value of one square.\"\"\"\n",
    "        self.Matrix[x, y] = val\n",
    "\n",
    "    def empties(self):\n",
    "        \"\"\"Return a 2d numpy array with the location of empty squares.\"\"\"\n",
    "        return np.argwhere(self.Matrix == 0)\n",
    "\n",
    "    def highest(self):\n",
    "        \"\"\"Report the highest tile on the board.\"\"\"\n",
    "        return np.max(self.Matrix)\n",
    "\n",
    "    def board_total(self):\n",
    "        \"\"\"Calculate the total value of all tiles on the board.\"\"\"\n",
    "        return np.sum(self.Matrix)\n",
    "\n",
    "\n",
    "    def move(self, direction, trial=False):\n",
    "        \"\"\"Perform one move of the game. Shift things to one side then,\n",
    "        combine. directions 0, 1, 2, 3 are up, right, down, left.\n",
    "        Returns the maximum score that [would have] got from the move.\"\"\"\n",
    "        if not trial:\n",
    "            if direction == 0:\n",
    "                logging.debug(\"Up\")\n",
    "            elif direction == 1:\n",
    "                logging.debug(\"Right\")\n",
    "            elif direction == 2:\n",
    "                logging.debug(\"Down\")\n",
    "            elif direction == 3:\n",
    "                logging.debug(\"Left\")\n",
    "\n",
    "        changed = False\n",
    "        scores = []  # 修改为列表，用于存储每次移动得到的分数\n",
    "        dir_div_two = int(direction / 2)\n",
    "        dir_mod_two = int(direction % 2)\n",
    "        shift_direction = dir_mod_two ^ dir_div_two  # 0 for towards up left, 1 for towards bottom right\n",
    "\n",
    "        # Construct a range for extracting row/column into a list\n",
    "        rx = list(range(self.w))\n",
    "        ry = list(range(self.h))\n",
    "\n",
    "        if dir_mod_two == 0:\n",
    "            # Up or down, split into columns\n",
    "            for y in range(self.h):\n",
    "                old = [self.get(x, y) for x in rx]\n",
    "                (new, ms) = self.shift(old, shift_direction)\n",
    "                scores.append(ms)  # 添加到分数列表中\n",
    "                if old != new:\n",
    "                    changed = True\n",
    "                    if not trial:\n",
    "                        for x in rx:\n",
    "                            self.set(x, y, new[x])\n",
    "        else:\n",
    "            # Left or right, split into rows\n",
    "            for x in range(self.w):\n",
    "                old = [self.get(x, y) for y in ry]\n",
    "                (new, ms) = self.shift(old, shift_direction)\n",
    "                scores.append(ms)  # 添加到分数列表中\n",
    "                if old != new:\n",
    "                    changed = True\n",
    "                    if not trial:\n",
    "                        for y in ry:\n",
    "                            self.set(x, y, new[y])\n",
    "        if not changed:\n",
    "            raise IllegalMove\n",
    "            # 打印分数列表和最大分数\n",
    "\n",
    "        # 获取列表中的最大值作为 move_scores 返回\n",
    "        move_scores = max(scores) if scores else 0\n",
    "        # print(\"Scores from this move:\", scores)\n",
    "        # print(\"Maximum score from this move:\", move_scores)\n",
    "        return move_scores\n",
    "\n",
    "    def combine(self, shifted_row):\n",
    "        \"\"\"Combine same tiles when moving to one side. This function always\n",
    "           shifts towards the left. Also count the score of combined tiles.\"\"\"\n",
    "        move_score = 0\n",
    "        combined_row = [0] * self.size\n",
    "        skip = False\n",
    "        output_index = 0\n",
    "        for p in pairwise(shifted_row):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            combined_row[output_index] = p[0]\n",
    "            if p[0] == p[1]:\n",
    "                combined_row[output_index] += p[1]\n",
    "                move_score += p[0] + p[1]\n",
    "                # Skip the next thing in the list.\n",
    "                skip = True\n",
    "            output_index += 1\n",
    "        if shifted_row and not skip:\n",
    "            combined_row[output_index] = shifted_row[-1]\n",
    "\n",
    "        return (combined_row, move_score)\n",
    "\n",
    "    def shift(self, row, direction):\n",
    "        \"\"\"Shift one row left (direction == 0) or right (direction == 1), combining if required.\"\"\"\n",
    "        length = len(row)\n",
    "        assert length == self.size\n",
    "        assert direction == 0 or direction == 1\n",
    "\n",
    "        # Shift all non-zero digits up\n",
    "        shifted_row = [i for i in row if i != 0]\n",
    "\n",
    "        # Reverse list to handle shifting to the right\n",
    "        if direction:\n",
    "            shifted_row.reverse()\n",
    "\n",
    "        (combined_row, move_score) = self.combine(shifted_row)\n",
    "\n",
    "        # Reverse list to handle shifting to the right\n",
    "        if direction:\n",
    "            combined_row.reverse()\n",
    "\n",
    "        assert len(combined_row) == self.size\n",
    "        return (combined_row, move_score)\n",
    "\n",
    "    def isend(self):\n",
    "        \"\"\"Has the game ended. Game ends if there is a tile equal to the limit\n",
    "           or there are no legal moves. If there are empty spaces then there\n",
    "           must be legal moves.\"\"\"\n",
    "\n",
    "        # if self.max_tile is not None and self.highest() == self.max_tile:\n",
    "        #     return True\n",
    "\n",
    "        for direction in range(4):\n",
    "            try:\n",
    "                self.move(direction, trial=True)\n",
    "                # Not the end if we can do any move\n",
    "                return False\n",
    "            except IllegalMove:\n",
    "                pass\n",
    "        return True\n",
    "\n",
    "    def get_board(self):\n",
    "        \"\"\"Retrieve the whole board, useful for testing.\"\"\"\n",
    "        return self.Matrix\n",
    "\n",
    "    def set_board(self, new_board):\n",
    "        \"\"\"Retrieve the whole board, useful for testing.\"\"\"\n",
    "        self.Matrix = new_board\n",
    "\n",
    "## 定义DQN算法\n",
    "# 定义用于低奖励情况的神经网络\n",
    "class NetLowReward(nn.Module):\n",
    "    def __init__(self, obs, available_actions_count):\n",
    "        super(NetLowReward, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(obs, 128, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=2, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 16, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(16, available_actions_count)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.fc1(x.view(x.shape[0], -1))\n",
    "        return x\n",
    "\n",
    "# 定义用于高奖励情况的神经网络\n",
    "class NetHighReward(nn.Module):\n",
    "    def __init__(self, obs, available_actions_count):\n",
    "        super(NetHighReward, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(obs, 128, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=2, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 16, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(16, available_actions_count)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.fc1(x.view(x.shape[0], -1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, args, obs_dim, action_dim, net, memory):\n",
    "        self.args = args\n",
    "        self.behaviour_model = net(obs_dim, action_dim).to(device)\n",
    "        # self.behaviour_model = torch.load(\"512-rollout-model.pt\").to(device)  # 加载并使用已训练的模型\n",
    "        self.target_model = net(obs_dim, action_dim).to(device)\n",
    "        # self.target_model =  torch.load(\"512-rollout-model.pt\").to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.behaviour_model.parameters(), args.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.action_dim = action_dim\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = memory\n",
    "        self.post_full_learn_counter = 0  # 初始化达到最大容量后的学习计数器\n",
    "        self.states_for_cosine_similarity = []  # 存储用于余弦相似度计算的状态\n",
    "        self.q_values_for_cosine_similarity = []  # 存储这些状态的旧 Q-\n",
    "        self.new_q_values_for_cosine_similarity=[]  # 更新列表2中的Q值\n",
    "        self.global_learn_counter = 0\n",
    "        self.save_graph_counter = 0  # 新的计数器\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        # 当经验池未满时，按原有逻辑进行学习\n",
    "        if self.memory.size < self.memory.buffer_size:\n",
    "            if self.memory.size <= 5000 and self.memory.size >= self.args.batch_size:\n",
    "                # 从经验池中随机抽取batch_size个样本进行学习\n",
    "                s1, a, s2, done, r = self.memory.get_sample(self.args.batch_size)\n",
    "                self.update_model(s1, a, s2, done, r)\n",
    "            elif self.memory.size > 5000 and self.memory.size % 5000 == 0:\n",
    "                for _ in range(80):  # 进行80次学习\n",
    "                    s1, a, s2, done, r = self.memory.get_sample(self.args.batch_size)\n",
    "                    self.update_model(s1, a, s2, done, r)\n",
    "\n",
    "        # 当经验池达到或超过最大容量时，每隔5000步进行80次学习\n",
    "        else:\n",
    "            self.post_full_learn_counter += 1  # 更新计数器\n",
    "            if self.post_full_learn_counter % 5000 == 0:\n",
    "                for _ in range(80):  # 80\n",
    "                    s1, a, s2, done, r = self.memory.get_sample(self.args.batch_size)\n",
    "                    self.update_model(s1, a, s2, done, r)\n",
    "\n",
    "        # 第一次经验池大小达到4999时，存储100个状态及其Q值\n",
    "        if self.memory.size == 4999:\n",
    "            sampled_indices = np.random.choice(range(self.memory.size), 100, replace=False)\n",
    "            states = torch.FloatTensor(self.memory.s1[sampled_indices]).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.behaviour_model(states)\n",
    "            self.states_for_cosine_similarity = states\n",
    "            self.q_values_for_cosine_similarity = q_values\n",
    "            # print('这是旧的',q_values)\n",
    "\n",
    "        # 当经验池大于4999但未满，或者经验池已满时，每隔5000步进行更新q\n",
    "        if self.memory.size > 4999:\n",
    "            # 更新全局计数器\n",
    "            self.global_learn_counter += 1\n",
    "            if self.global_learn_counter % 5000 == 0:\n",
    "                # 重新计算Q值\n",
    "                with torch.no_grad():\n",
    "                    new_q_values = self.behaviour_model(self.states_for_cosine_similarity)\n",
    "                # 更新列表2中的Q值\n",
    "                self.new_q_values_for_cosine_similarity = new_q_values\n",
    "                print(new_q_values)\n",
    "                # # 计算并存储余弦相似度\n",
    "                # cosine_similarity = self.calculate_cosine_similarity()\n",
    "\n",
    "\n",
    "    def update_model(self, s1, a, s2, done, r):\n",
    "        s1 = torch.FloatTensor(s1).to(device)\n",
    "        s2 = torch.FloatTensor(s2).to(device)\n",
    "        r = torch.FloatTensor(r).to(device)\n",
    "        a = torch.LongTensor(a).to(device)\n",
    "        done = torch.FloatTensor(done).to(device)\n",
    "\n",
    "        if self.learn_step_counter % self.args.target_update_freq == 0:\n",
    "            self.target_model.load_state_dict(self.behaviour_model.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        if self.args.use_nature_dqn:\n",
    "            q = self.target_model(s2).detach()\n",
    "        else:\n",
    "            q = self.behaviour_model(s2)\n",
    "\n",
    "        target_q = r + (self.args.gamma * (1 - done)).to(device) * q.max(1)[0]\n",
    "        eval_q = self.behaviour_model(s1).gather(1, a.view(-1, 1))\n",
    "        target_q = target_q.view(-1, 1)\n",
    "        loss = self.criterion(eval_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_with_new_q(self, state, new_q):\n",
    "        # 将状态转换为张量格式\n",
    "        state_tensor = torch.FloatTensor(np.expand_dims(state, axis=0)).to(device)\n",
    "        # 计算网络当前的 Q 值\n",
    "        current_q_values = self.behaviour_model(state_tensor)\n",
    "        # 将新的 Q 值转换为张量格式\n",
    "        new_q_values = torch.FloatTensor(new_q).to(device).view_as(current_q_values)\n",
    "        # 计算损失并更新网络\n",
    "        loss = self.criterion(current_q_values, new_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def calculate_cosine_similarity(self):\n",
    "        # 使用更新后的模型计算新的 Q-值\n",
    "        with torch.no_grad():\n",
    "            new_q_values = self.behaviour_model(self.states_for_cosine_similarity)\n",
    "\n",
    "        # 计算余弦相似度\n",
    "        cosine_similarities = []\n",
    "        for old_q, new_q in zip(self.q_values_for_cosine_similarity, new_q_values):\n",
    "            cosine_similarity = 1 - cosine(old_q.cpu().numpy(), new_q.cpu().numpy())\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "        average_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
    "        return average_cosine_similarity\n",
    "\n",
    "    def calculate_normalized_distance(self):\n",
    "        # 计算新旧Q值之间的归一化欧几里得距离\n",
    "        normalized_distances = []\n",
    "        with torch.no_grad():\n",
    "            new_q_values = self.behaviour_model(self.states_for_cosine_similarity)\n",
    "\n",
    "        for old_q, new_q in zip(self.q_values_for_cosine_similarity, new_q_values):\n",
    "            # print('这是旧',old_q)\n",
    "            # print('这是新',new_q)\n",
    "            distance = torch.norm(new_q - old_q).item() / torch.norm(old_q).item()\n",
    "            normalized_distances.append(distance)\n",
    "\n",
    "        average_distance = sum(normalized_distances) / len(normalized_distances)\n",
    "        return average_distance\n",
    "\n",
    "    def get_action(self, state, explore=True):\n",
    "        if explore:\n",
    "            # 使用模型预测动作的概率分布\n",
    "            logits = self.behaviour_model(torch.FloatTensor(state).to(device))\n",
    "            policy = F.softmax(logits, dim=1)\n",
    "            m = Categorical(policy)\n",
    "            # 随机选择一个动作，基于模型的概率分布\n",
    "            action = m.sample().item()\n",
    "        else:\n",
    "            # 如果不探索，直接选择最佳动作\n",
    "            with torch.no_grad():\n",
    "                q = self.behaviour_model(torch.FloatTensor(state).to(device))\n",
    "                _, action = torch.max(q, 1)\n",
    "                action = action.item()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def test_directions(self, special_buffer, model, num_trials, target_score=512):\n",
    "        all_states = special_buffer.get_all_states_from_b_to_pos()\n",
    "        # 去重处理\n",
    "        unique_states = []\n",
    "        for state in all_states:\n",
    "            if not any(np.array_equal(state, unique_state) for unique_state in unique_states):\n",
    "                unique_states.append(state)\n",
    "\n",
    "        # print(\"去重后的状态数量:\", len(unique_states))\n",
    "\n",
    "        direction_results = {\"up\": [], \"down\": [], \"left\": [], \"right\": []}\n",
    "        found_target = False\n",
    "        key_state = None  # 用于存储关键状态\n",
    "        key_direction = None  # 新增变量记录关键方向\n",
    "\n",
    "        # 反向遍历所有去重后的状态\n",
    "        for state in reversed(unique_states):\n",
    "            unstacked_state = unstack(state)\n",
    "            # print('检查的状态:', unstacked_state)\n",
    "            s=unstacked_state.copy()\n",
    "\n",
    "            temp_results = {\"up\": None, \"down\": None, \"left\": None, \"right\": None}\n",
    "\n",
    "            # 检查每个方向\n",
    "            for direction in temp_results:\n",
    "                # print('检查方向:', direction)\n",
    "                avg_rounds, reached_target, target_direction = self.run_trials(unstacked_state, model, direction, num_trials, target_score)\n",
    "\n",
    "                temp_results[direction] = avg_rounds\n",
    "\n",
    "                if reached_target and not found_target:\n",
    "                    found_target = True\n",
    "                    key_state = s\n",
    "                    key_direction = target_direction\n",
    "\n",
    "\n",
    "            if found_target:\n",
    "                # 如果找到关键节点，将临时结果复制到最终结果中\n",
    "                for direction in direction_results:\n",
    "                    direction_results[direction] = [temp_results[direction]]\n",
    "                break\n",
    "\n",
    "        if not found_target:\n",
    "            # 如果没有找到目标分数，将所有方向的分数设置为0\n",
    "            for direction in direction_results:\n",
    "                direction_results[direction] = [0]\n",
    "\n",
    "        return direction_results, key_state, key_direction\n",
    "\n",
    "    def run_trials(self, initial_state, model, direction, num_trials, target_score):\n",
    "        total_rounds = 0\n",
    "        reached_target = False\n",
    "\n",
    "        for _ in range(num_trials):\n",
    "            # 重置环境到初始状态\n",
    "            env.Matrix = initial_state\n",
    "            # print('初始状态:', initial_state)\n",
    "            env.score = 0\n",
    "            s = stack(initial_state)\n",
    "            rounds = 0\n",
    "            done = False\n",
    "\n",
    "            # 执行第一步\n",
    "            s, _, done, _, _, _ = env.step(self.direction_to_action(direction))\n",
    "            last_s = None\n",
    "\n",
    "            while not done:\n",
    "                s_tensor = torch.FloatTensor(np.expand_dims(s, axis=0)).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q = model(s_tensor)\n",
    "                    a = torch.argmax(q, 1).item()\n",
    "                s, _, done, _, _, mm = env.step(a)\n",
    "                if env.highest() > target_score:\n",
    "                    reached_target = True\n",
    "\n",
    "                if np.array_equal(s, last_s):\n",
    "                    break\n",
    "                last_s = s.copy()\n",
    "                rounds += 1\n",
    "\n",
    "            total_rounds += rounds\n",
    "\n",
    "        return total_rounds / num_trials, reached_target,direction\n",
    "\n",
    "    def direction_to_action(self, direction):\n",
    "        action = {\"up\": 0, \"right\": 1,\"down\": 2, \"left\": 3}[direction]\n",
    "        # print(f\"Direction: {direction}, Action: {action}\")\n",
    "        return action\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, obs_space):\n",
    "        self.s1 = np.zeros(obs_space, dtype=np.float32)\n",
    "        self.s2 = np.zeros(obs_space, dtype=np.float32)\n",
    "        self.a = np.zeros(buffer_size, dtype=np.int32)\n",
    "        self.r = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.done = np.zeros(buffer_size, dtype=np.float32)\n",
    "\n",
    "        # replaybuffer大小\n",
    "        self.buffer_size = buffer_size\n",
    "        self.size = 0\n",
    "        self.pos = 0\n",
    "\n",
    "    # 不断将数据存储入buffer\n",
    "    def add_transition(self, s1, action, s2, done, reward):\n",
    "        self.s1[self.pos] = s1\n",
    "        self.a[self.pos] = action\n",
    "        if not done:\n",
    "            self.s2[self.pos] = s2\n",
    "        self.done[self.pos] = done\n",
    "        self.r[self.pos] = reward\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.buffer_size\n",
    "        self.size = min(self.size + 1, self.buffer_size)\n",
    "\n",
    "    # 随机采样一个batchsize\n",
    "    def get_sample(self, sample_size):\n",
    "        i = sample(range(0, self.size), sample_size)\n",
    "        return self.s1[i], self.a[i], self.s2[i], self.done[i], self.r[i]\n",
    "\n",
    "        # 清空数据\n",
    "\n",
    "    def clear(self):\n",
    "        self.s1.fill(0)\n",
    "        self.s2.fill(0)\n",
    "        self.a.fill(0)\n",
    "        self.r.fill(0)\n",
    "        self.done.fill(0)\n",
    "        self.size = 0\n",
    "        self.pos = 0\n",
    "\n",
    "\n",
    "class SpecialReplayBuffer:\n",
    "    def __init__(self, buffer_size, obs_space):\n",
    "        self.s1 = np.zeros(obs_space, dtype=np.float32)\n",
    "        self.pos = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.size = 0\n",
    "        self.pointer_b_updated = False  # 用于标记是否已更新\n",
    "\n",
    "    def add_state(self, s1):\n",
    "        self.s1[self.pos] = s1\n",
    "        self.pos = (self.pos + 1) % self.buffer_size\n",
    "        self.size = min(self.size + 1, self.buffer_size)\n",
    "        if not self.pointer_b_updated:\n",
    "            self.pointer_b = self.pos\n",
    "            self.pointer_b_updated = True  # 标记已更新\n",
    "\n",
    "    def reset_pointer_b(self):\n",
    "        self.pointer_b_updated = False  # 重置标记\n",
    "\n",
    "    def get_last_states(self, n):\n",
    "        print(\"当前指针位置:\", self.pos)\n",
    "        indices = [(self.pos - i - 1) % self.buffer_size for i in range(n)]\n",
    "        return self.s1[indices]\n",
    "\n",
    "    def get_first_states(self, n):\n",
    "        print(\"当前指针位置:\", self.pointer_b)\n",
    "        if self.size < n:\n",
    "            return self.s1[:self.size]  # 如果存储的状态少于n个，返回所有状态\n",
    "        start_index = self.pointer_b\n",
    "        indices = [(start_index + i) % self.buffer_size for i in range(n)]  # 计算需要返回的状态的索引\n",
    "        return self.s1[indices]  # 返回这些状态\n",
    "\n",
    "\n",
    "    def get_all_states_from_b_to_pos(self):\n",
    "        if self.pointer_b <= self.pos:\n",
    "            # 如果头指针在尾指针之前或相等，则直接返回这个范围内的状态\n",
    "            return self.s1[self.pointer_b:self.pos]\n",
    "        else:\n",
    "            # 如果头指针在尾指针之后，则需要分两部分返回\n",
    "            return np.concatenate((self.s1[self.pointer_b:], self.s1[:self.pos]), axis=0)\n",
    "\n",
    "# 测试模型函数:\n",
    "def infer_model(model, env, threshold=1024, num_trials=100):\n",
    "    random_action_count = 0\n",
    "    model_action_count = 0\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        s = env.reset()\n",
    "        last_s = None\n",
    "        actions_taken = []  # 初始化已尝试的动作列表\n",
    "\n",
    "        while True:\n",
    "            s_tensor = torch.FloatTensor(np.expand_dims(s, axis=0)).to(device)\n",
    "            if np.array_equal(last_s, s):\n",
    "                # 去除已尝试的动作\n",
    "                available_actions = [i for i in range(4) if i not in actions_taken]\n",
    "\n",
    "                # 如果没有剩余动作尝试，结束当前尝试\n",
    "                if not available_actions:\n",
    "                    break\n",
    "\n",
    "                # 选择一个新的动作\n",
    "                a = random.choice(available_actions)\n",
    "                actions_taken.append(a)  # 记录尝试过的动作\n",
    "                random_action_count += 1\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    logits = model(s_tensor)\n",
    "                    policy = F.softmax(logits, dim=1)\n",
    "                    m = Categorical(policy)\n",
    "                    a = m.sample().item()\n",
    "                model_action_count += 1\n",
    "                # 重置动作列表\n",
    "                actions_taken = []\n",
    "\n",
    "            last_s = np.array(s)\n",
    "            s_, r, done, info = env.step(a)\n",
    "            if done:\n",
    "                if env.highest() >= threshold:\n",
    "                    success_count += 1\n",
    "                break\n",
    "            s = s_\n",
    "    return success_count, random_action_count, model_action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 胜率绘制函数\n",
    "def plot_success_rate(success_rate_iterations, success_rates, filename):\n",
    "    # 绘制 Success Rate\n",
    "    plt.plot(success_rate_iterations, success_rates, marker='o', label='Success Rate')\n",
    "    for i, txt in enumerate(success_rates):\n",
    "        plt.text(success_rate_iterations[i], success_rates[i], f\"{txt:.3f}\")\n",
    "\n",
    "    # 设置图表标题和坐标轴标签\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Max Reward \")\n",
    "    plt.title(\"Max Reward, c Value. Iteration\")\n",
    "\n",
    "    # 显示图例和网格\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # 保存图表\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. 训练模型\n",
    "\n",
    "# 初始化环境\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = Game2048Env()\n",
    "# 初始化replay buffer\n",
    "#replay_buffer_low = ReplayBuffer(buffer_size=args.buffer_size, obs_space=(args.buffer_size, env.observation_space.shape[0], env.observation_space.shape[1], env.observation_space.shape[2]))\n",
    "replay_buffer_high = ReplayBuffer(buffer_size=args.buffer_size, obs_space=(args.buffer_size, env.observation_space.shape[0], env.observation_space.shape[1], env.observation_space.shape[2]))\n",
    "# filter_pool= ReplayBuffer(buffer_size=10000, obs_space=(args.buffer_size, env.observation_space.shape[0], env.observation_space.shape[1], env.observation_space.shape[2]))\n",
    "\n",
    "\n",
    "# 为低奖励和高奖励情况分别初始化 DQN 实例\n",
    "#DQN_low = DQN(args, obs_dim=env.observation_space.shape[2], action_dim=env.action_space.n, net=NetLowReward, memory=replay_buffer_low)\n",
    "DQN_high = DQN(args, obs_dim=env.observation_space.shape[2], action_dim=env.action_space.n, net=NetHighReward, memory=replay_buffer_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024胜率: 26.40%\n",
      "2048胜率: 0.20%\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "with open(\"Test4.pkl\", 'rb') as file:\n",
    "    model_data = pickle.load(file)\n",
    "\n",
    "best_model = model_data['model']  # 加载最佳模型\n",
    "replay_buffer = model_data['replay_buffer']  # 可选，若需要回放缓存\n",
    "\n",
    "# 验证模型在 1024 和 2048 分数阈值下的表现\n",
    "success_count_1024, _, _ = infer_model(best_model, env, threshold=1024, num_trials=1000)\n",
    "success_rate_1024 = success_count_1024 / 1000\n",
    "print(f\"1024胜率: {success_rate_1024:.2%}\")\n",
    "\n",
    "success_count_2048, _, _ = infer_model(best_model, env, threshold=2048, num_trials=1000)\n",
    "success_rate_2048 = success_count_2048 / 1000\n",
    "print(f\"2048胜率: {success_rate_2048:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024胜率: 46.10%\n",
      "2048胜率: 1.10%\n"
     ]
    }
   ],
   "source": [
    "# 加载模型 46%\n",
    "with open(\"Test2.pkl\", 'rb') as file:\n",
    "    model_data = pickle.load(file)\n",
    "\n",
    "best_model = model_data['model']  # 加载最佳模型\n",
    "replay_buffer = model_data['replay_buffer']  # 可选，若需要回放缓存\n",
    "\n",
    "# 验证模型在 1024 和 2048 分数阈值下的表现\n",
    "success_count_1024, _, _ = infer_model(best_model, env, threshold=1024, num_trials=1000)\n",
    "success_rate_1024 = success_count_1024 / 1000\n",
    "print(f\"1024胜率: {success_rate_1024:.2%}\")\n",
    "\n",
    "success_count_2048, _, _ = infer_model(best_model, env, threshold=2048, num_trials=1000)\n",
    "success_rate_2048 = success_count_2048 / 1000\n",
    "print(f\"2048胜率: {success_rate_2048:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024胜率: 62.60%\n",
      "2048胜率: 4.50%\n"
     ]
    }
   ],
   "source": [
    "# 加载模型 62%\n",
    "with open(\"Test3.pkl\", 'rb') as file:\n",
    "    model_data = pickle.load(file)\n",
    "\n",
    "best_model = model_data['model']  # 加载最佳模型\n",
    "replay_buffer = model_data['replay_buffer']  # 可选，若需要回放缓存\n",
    "\n",
    "# 验证模型在 1024 和 2048 分数阈值下的表现\n",
    "success_count_1024, _, _ = infer_model(best_model, env, threshold=1024, num_trials=1000)\n",
    "success_rate_1024 = success_count_1024 / 1000\n",
    "print(f\"1024胜率: {success_rate_1024:.2%}\")\n",
    "\n",
    "success_count_2048, _, _ = infer_model(best_model, env, threshold=2048, num_trials=1000)\n",
    "success_rate_2048 = success_count_2048 / 1000\n",
    "print(f\"2048胜率: {success_rate_2048:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024胜率: 60.70%\n",
      "2048胜率: 5.20%\n"
     ]
    }
   ],
   "source": [
    "# 加载模型 62%\n",
    "with open(\"Test3.pkl\", 'rb') as file:\n",
    "    model_data = pickle.load(file)\n",
    "\n",
    "best_model = model_data['model']  # 加载最佳模型\n",
    "replay_buffer = model_data['replay_buffer']  # 可选，若需要回放缓存\n",
    "\n",
    "# 验证模型在 1024 和 2048 分数阈值下的表现\n",
    "success_count_1024, _, _ = infer_model(best_model, env, threshold=1024, num_trials=1000)\n",
    "success_rate_1024 = success_count_1024 / 1000\n",
    "print(f\"1024胜率: {success_rate_1024:.2%}\")\n",
    "\n",
    "success_count_2048, _, _ = infer_model(best_model, env, threshold=2048, num_trials=1000)\n",
    "success_rate_2048 = success_count_2048 / 1000\n",
    "print(f\"2048胜率: {success_rate_2048:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024胜率: 64.90%\n",
      "2048胜率: 7.10%\n"
     ]
    }
   ],
   "source": [
    "# 加载模型 62%\n",
    "with open(\"Test3.pkl\", 'rb') as file:\n",
    "    model_data = pickle.load(file)\n",
    "\n",
    "best_model = model_data['model']  # 加载最佳模型\n",
    "# replay_buffer = model_data['replay_buffer']  # 可选，若需要回放缓存\n",
    "\n",
    "# 验证模型在 1024 和 2048 分数阈值下的表现\n",
    "success_count_1024, _, _ = infer_model(best_model, env, threshold=1024, num_trials=1000)\n",
    "success_rate_1024 = success_count_1024 / 1000\n",
    "print(f\"1024胜率: {success_rate_1024:.2%}\")\n",
    "\n",
    "success_count_2048, _, _ = infer_model(best_model, env, threshold=2048, num_trials=1000)\n",
    "success_rate_2048 = success_count_2048 / 1000\n",
    "print(f\"2048胜率: {success_rate_2048:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
